# content
for (i in 1:length(docBC_ids)) {
print(inspect(co[[docBC_ids[i]]]))
}
###################################################################################
# Documents without principal phrases
###################################################################################
colFreq=apply(pdm,2,sum)
sum(colFreq==0)
# 3 Documents
# I will explore these later
# Documents
###################################################################################
# Best Document
###################################################################################
## for 5  hf Phrases ###
bd=bestDocs(co,nDocs = 10,nPhrases = 5, STOP_PHRASES)
bd
oldID = 0
hfPhrases = 0
for (i in 1:length(bd)) {
oldID[i] = meta(bd[[i]],"oldID")
hfPhrases[i] = meta(bd[[i]],"hfPhrases")
}
oldID
hfPhrases
(topDocIndicsAndFreq = cbind(oldID,hfPhrases))
## looking at index
# using vectors
(id = oldID[1]) # __ index
hfPhrases[1] # __ hf phrases
# Checking this docuemnt in original corpus
inspect(co[[id]])
meta(co[[id]])
# All principal phrases
pdm[,id] != 0
pdm[pdm[,id] != 0,id, drop=F]
# comapring to top high frequency phrases
freqPhrases(pd,5)
## for 10 hf Phrases
bd=bestDocs(co,nDocs = 10,nPhrases = 10, STOP_PHRASES)
bd
oldID = 0
hfPhrases = 0
for (i in 1:length(bd)) {
oldID[i] = meta(bd[[i]],"oldID")
hfPhrases[i] = meta(bd[[i]],"hfPhrases")
}
oldID
hfPhrases
(topDocIndicsAndFreq = cbind(oldID,hfPhrases))
## looking at index
# using vectors
(id = oldID[1]) # __ index
hfPhrases[1] # __ hf phrases
# Checking this docuemnt in original corpus
inspect(co[[id]])
meta(co[[id]])
# All principal phrases
pdm[,id] != 0
pdm[pdm[,id] != 0,id, drop=F]
# comparing to top high frequency phrases
freqPhrases(pd,10)
ChosenDocID = id
###################################################################################
# Plot the frequent phrases in the Chosen Document of the documents
###################################################################################
ChosenDocPdm = pdm[pdm[,ChosenDocID]!=0,ChosenDocID,drop=F]      #Frequent phrases in document
#Word cloud 1
set.seed(123)
unique(ChosenDocPdm)
myCol=c("darkgreen","blue","red")
wordcloud(row.names(ChosenDocPdm),
ChosenDocPdm,
min.freq=1,
colors=myCol,
random.order=F,scale=c(3,.3))
#Word cloud2
set.seed(123)
wordcloud2(data.frame(word=row.names(ChosenDocPdm),freq=ChosenDocPdm),size=.3)
# try doing sentient analysis on this document
###################################################################################
# Sentiment Analysis of the Doc
###################################################################################
co[ChosenDocID]
co[ChosenDocID]
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
tdM
tdM=as.matrix(TermDocumentMatrix(co[ChosenDocID], control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
tdM
#b
tdMdf=as.data.frame(tdM)
tdMdf$word=rownames(tdMdf)
tdMdf
tdMdf$word=rownames(tdMdf)
#Term document data frame for words in afinn lexicon
tdMdf=inner_join(tdMdf,get_sentiments("afinn"))
tdMdf
ptd=co[ChosenDocID]    #First document in the corpus
ptd
tf=termFreq(ptd,control=list(removePunctuation=T,stopwords=T,
removeNumbers=T))
ptd
tf=termFreq(ptd,control=list(removePunctuation=T,stopwords=T,
removeNumbers=T))
tf=termFreq(ptd,control=list(removePunctuation=T,
stopwords=T,
removeNumbers=T))
#b
tdMdf=as.data.frame(tdM)
tf=termFreq(ptd,control=list(removePunctuation=T,
stopwords=T,
removeNumbers=T))
ptd
ptd=co[[ChosenDocID]]    #First document in the corpus
ptd
tf=termFreq(ptd,control=list(removePunctuation=T,
stopwords=T,
removeNumbers=T))
tf
#The joins need a data frame of this format
df=data.frame(word=names(tf),freq=tf)
source('~/GitHub/Network-and-Text-Analytics-/Project Text Mining.R', echo=TRUE)
inner_join(df,get_sentiments("bing"))
inner_join(df,get_sentiments("afinn"))
inner_join(df,get_sentiments("bing"))
#Select all the fear words
fearnrc = nrc[nrc$sentiment == 'fear',]
ptdFear = inner_join(df,fearnrc)
#Select all the fear words
fearnrc = nrc[nrc$sentiment == 'fear',]
nrc
#Select all the fear words
nrc$sentiment
#Select all the fear words
(nrc=get_sentiments("nrc"))
fearnrc = nrc[nrc$sentiment == 'fear',]
#par(parSave)
###################################################################################
# Processing
###################################################################################
#1
co=VCorpus(PubMedSource(toDataFrame("TextData/tmData/Covid19March1.txt")))
#a
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
###################################################################################
#
# HW11.R
#
###################################################################################
# External Functions
###################################################################################
library(tm)
library(tidytext)          #For sentiments
library(dplyr)             #For joins
library(wordcloud)
library(wordcloud2)
library(sur)               #Skewness ratios
source("R-Code/Text Analysis/toDataFrame.R")
###################################################################################
# Internal Functions
###################################################################################
PubMedSource<-function (x)
{
stopifnot(all(!is.na(match(c("PMID", "Title","Abstract","Date","Author"),
names(x)))))
SimpleSource(length = nrow(x), reader = readPub, content = x,
class = "PubMedSource")
}
readPub<-function(elem, language, id) {
PlainTextDocument(elem$content[,"Abstract"], id=elem$content[,"PMID"],
language=language, author=elem$content[,"Author"],
datetimestamp=elem$content[,"Date"],
title=elem$content[,"Title"])
}
getElem.PubMedSource<-function (x)
list(content = x$content[x$position, ], uri = NULL)
###################################################################################
# Save the environment
###################################################################################
parSave=par(no.readonly = TRUE)
#par(parSave)
###################################################################################
# Processing
###################################################################################
#1
co=VCorpus(PubMedSource(toDataFrame("TextData/tmData/Covid19March1.txt")))
#a
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
#tdM=as.matrix(TermDocumentMatrix(co))
tdM[1:5,1:5]
#b
tdMdf=as.data.frame(tdM)
tdMdf
tdMdf$word=rownames(tdMdf)
rownames(tdMdf)
tdMdf$word
tdMdf
tdMdf[1:5,1:5]
head(tdMdf)
#Term document data frame for words in afinn lexicon
tdMdf=inner_join(tdMdf,get_sentiments("afinn"))
tdMdf
rownames(tdMdf)=tdMdf$word
tdMdf$word=NULL
dim(tdMdf)              #204x184
tdMdf[1:5,180:184]
#c
#Find indices for documents that have frequencies for these words
sumFreq=apply(tdMdf[,1:(ncol(tdMdf)-1)],2,sum)
sumFreq
docs=which(sumFreq!=0)
length(docs)            #139
#c
#Find indices for documents that have frequencies for these words
sumFreq=apply(tdMdf[,1:(ncol(tdMdf)-1)],2,sum)
docs=which(sumFreq!=0)
length(docs)            #139
docs
tdMdf
#e
#For document 1, find frequencies and sentiments
tdMdf[tdMdf[,1]!=0,c(1,ncol(tdMdf))]
#Frequencies times sentiment for document 1
tdMdf2[tdMdf2[,1]!=0,1,drop=F]
#For document 3
tdMdf[tdMdf[,3]!=0,c(3,ncol(tdMdf))]
tdMdf2[tdMdf2[,3]!=0,3,drop=F]
#f
#Add all the sentiments by document
sumSent=apply(tdMdf2,2,sum)
sumSent
###################################################################################
#
# HW11.R
#
###################################################################################
# External Functions
###################################################################################
library(tm)
library(tidytext)          #For sentiments
library(dplyr)             #For joins
library(wordcloud)
library(wordcloud2)
library(sur)               #Skewness ratios
source("R-Code/Text Analysis/toDataFrame.R")
###################################################################################
# Internal Functions
###################################################################################
PubMedSource<-function (x)
{
stopifnot(all(!is.na(match(c("PMID", "Title","Abstract","Date","Author"),
names(x)))))
SimpleSource(length = nrow(x), reader = readPub, content = x,
class = "PubMedSource")
}
readPub<-function(elem, language, id) {
PlainTextDocument(elem$content[,"Abstract"], id=elem$content[,"PMID"],
language=language, author=elem$content[,"Author"],
datetimestamp=elem$content[,"Date"],
title=elem$content[,"Title"])
}
getElem.PubMedSource<-function (x)
list(content = x$content[x$position, ], uri = NULL)
###################################################################################
# Save the environment
###################################################################################
parSave=par(no.readonly = TRUE)
#par(parSave)
###################################################################################
# Processing
###################################################################################
#1
co=VCorpus(PubMedSource(toDataFrame("TextData/tmData/Covid19March1.txt")))
#a
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
#tdM=as.matrix(TermDocumentMatrix(co))
tdM[1:5,1:5]
#b
tdMdf=as.data.frame(tdM)
tdMdf$word=rownames(tdMdf)
head(tdMdf)
#Term document data frame for words in afinn lexicon
tdMdf=inner_join(tdMdf,get_sentiments("afinn"))
rownames(tdMdf)=tdMdf$word
tdMdf$word=NULL
dim(tdMdf)              #204x184
tdMdf[1:5,180:184]
#c
#Find indices for documents that have frequencies for these words
sumFreq=apply(tdMdf[,1:(ncol(tdMdf)-1)],2,sum)
docs=which(sumFreq!=0)
length(docs)            #139
#d
#Multiply frequency by sentiment
sentiment_value = tdMdf[,ncol(tdMdf)]
all_words = tdMdf[,-ncol(tdMdf)]
tdMdf2=all_words * sentiment_value
dim(tdMdf2)
#e
#For document 1, find frequencies and sentiments
tdMdf[tdMdf[,1]!=0,c(1,ncol(tdMdf))]
#Frequencies times sentiment for document 1
tdMdf2[tdMdf2[,1]!=0,1,drop=F]
#For document 3
tdMdf[tdMdf[,3]!=0,c(3,ncol(tdMdf))]
tdMdf2[tdMdf2[,3]!=0,3,drop=F]
#f
#Add all the sentiments by document
sumSent=apply(tdMdf2,2,sum)
#Scatterplot of the sentiments of the documents
plot(docs,sumSent[docs],xlab=NA,ylab="Sentiments",xaxt='n')
abline(h=0,col=2)
#g
hist(sumSent,col="violet",main="Sentiments by Document",xlab="Sentiment",
breaks=15)
#Most of the documents seem pretty neutral, but there are some
#(apparently very sad) outliers
skew.ratio(sumSent)
#Very negatively skewed
mean(sumSent[docs])
median(sumSent[docs])
#The documents in the corpus are typically neutral.
#h
(id1=which.max(sumSent))                    #Most positive document
sumSent[id1]                                #How positive?
tdMdf[tdMdf[,id1]!=0,c(id1,ncol(tdMdf))]
inspect(co[[id1]])
(id2=which.min(sumSent))                    #Most negative document
sumSent[id2]                                #How negative?
tdMdf[tdMdf[,id2]!=0,c(id2,ncol(tdMdf))]
inspect(co[[id2]])
#i
#Most upbeat document
tfPos=termFreq(co[[id1]],control=list(removePunctuation=T,stopwords=T,
removeNumbers=T))
set.seed(23)
wordcloud(names(tfPos),tfPos,col=c("black","blue","wheat","green","red"),
min.freq=1,scale=c(3,.2),random.order=F)
wordcloud2(data.frame(word=names(tfPos),freq=tfPos),size=.7,shape="square",
ellipticity=1)
#Most depressing document
tfNeg=termFreq(co[[id2]],control=list(removePunctuation=T,stopwords=T,
removeNumbers=T))
set.seed(23)
wordcloud(names(tfNeg),tfNeg,col=c("black","blue","green","purple","red"),min.freq=1,
scale=c(3,.2),random.order=F)
wordcloud2(data.frame(word=names(tfNeg),freq=tfNeg),size=.7)
sumSent
###################################################################################
#
# HW11.R
#
###################################################################################
# External Functions
###################################################################################
library(tm)
library(tidytext)          #For sentiments
library(dplyr)             #For joins
library(wordcloud)
library(wordcloud2)
library(sur)               #Skewness ratios
source("R-Code/Text Analysis/toDataFrame.R")
###################################################################################
# Internal Functions
###################################################################################
PubMedSource<-function (x)
{
stopifnot(all(!is.na(match(c("PMID", "Title","Abstract","Date","Author"),
names(x)))))
SimpleSource(length = nrow(x), reader = readPub, content = x,
class = "PubMedSource")
}
readPub<-function(elem, language, id) {
PlainTextDocument(elem$content[,"Abstract"], id=elem$content[,"PMID"],
language=language, author=elem$content[,"Author"],
datetimestamp=elem$content[,"Date"],
title=elem$content[,"Title"])
}
getElem.PubMedSource<-function (x)
list(content = x$content[x$position, ], uri = NULL)
###################################################################################
# Save the environment
###################################################################################
parSave=par(no.readonly = TRUE)
#par(parSave)
###################################################################################
# Processing
###################################################################################
#1
co=VCorpus(PubMedSource(toDataFrame("TextData/tmData/Covid19March1.txt")))
#a
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
#tdM=as.matrix(TermDocumentMatrix(co))
tdM[1:5,1:5]
#b
tdMdf=as.data.frame(tdM)
tdMdf$word=rownames(tdMdf)
head(tdMdf)
#Term document data frame for words in afinn lexicon
tdMdf=inner_join(tdMdf,get_sentiments("afinn"))
rownames(tdMdf)=tdMdf$word
tdMdf$word=NULL
dim(tdMdf)              #204x184
tdMdf[1:5,180:184]
#c
#Find indices for documents that have frequencies for these words
sumFreq=apply(tdMdf[,1:(ncol(tdMdf)-1)],2,sum)
sumFreq
class(sum)
class(sumFreq)
sumSent[docs]
#par(parSave)
###################################################################################
# Processing
###################################################################################
#1
co=VCorpus(PubMedSource(toDataFrame("TextData/tmData/Covid19March1.txt")))
#a
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
#tdM=as.matrix(TermDocumentMatrix(co))
tdM[1:5,1:5]
#b
tdMdf=as.data.frame(tdM)
tdMdf$word=rownames(tdMdf)
head(tdMdf)
#Term document data frame for words in afinn lexicon
tdMdf=inner_join(tdMdf,get_sentiments("afinn"))
rownames(tdMdf)=tdMdf$word
tdMdf$word=NULL
dim(tdMdf)              #204x184
tdMdf[1:5,180:184]
#c
#Find indices for documents that have frequencies for these words
sumFreq=apply(tdMdf[,1:(ncol(tdMdf)-1)],2,sum)
class(sumFreq)
docs=which(sumFreq!=0)
length(docs)            #139
#d
#Multiply frequency by sentiment
sentiment_value = tdMdf[,ncol(tdMdf)]
all_words = tdMdf[,-ncol(tdMdf)]
tdMdf2=all_words * sentiment_value
dim(tdMdf2)
#e
#For document 1, find frequencies and sentiments
tdMdf[tdMdf[,1]!=0,c(1,ncol(tdMdf))]
#Frequencies times sentiment for document 1
tdMdf2[tdMdf2[,1]!=0,1,drop=F]
#For document 3
tdMdf[tdMdf[,3]!=0,c(3,ncol(tdMdf))]
tdMdf2[tdMdf2[,3]!=0,3,drop=F]
#f
#Add all the sentiments by document
sumSent=apply(tdMdf2,2,sum)
sumSent
#f
#Add all the sentiments by document
sumSent=apply(tdMdf2,2,sum)
#Scatterplot of the sentiments of the documents
plot(docs,sumSent[docs],xlab=NA,ylab="Sentiments",xaxt='n')
abline(h=0,col=2)
#g
hist(sumSent,col="violet",main="Sentiments by Document",xlab="Sentiment",
breaks=15)
#Most of the documents seem pretty neutral, but there are some
#(apparently very sad) outliers
skew.ratio(sumSent)
mean(sumSent[docs])
median(sumSent[docs])
#h
(id1=which.max(sumSent))                    #Most positive document
#e
#For document 1, find frequencies and sentiments
tdMdf[tdMdf[,1]!=0,c(1,ncol(tdMdf))]
#Frequencies times sentiment for document 1
tdMdf2[tdMdf2[,1]!=0,1,drop=F]
#For document 3
tdMdf[tdMdf[,3]!=0,c(3,ncol(tdMdf))]
tdMdf2[tdMdf2[,3]!=0,3,drop=F]
ncol(tdMdf)-1
ncol(tdMdf)
#b
tdMdf=as.data.frame(tdM)
tdMdf$word=rownames(tdMdf)
head(tdMdf)
#Term document data frame for words in afinn lexicon
tdMdf=inner_join(tdMdf,get_sentiments("afinn"))
rownames(tdMdf)=tdMdf$word
tdMdf$word=NULL
dim(tdMdf)              #204x184
tdMdf[1:5,180:184]
#c
#Find indices for documents that have frequencies for these words
sumFreq=apply(tdMdf[,1:(ncol(tdMdf)-1)],2,sum)
docs=which(sumFreq!=0)
tdMdf
tdMdf2
